{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model \n",
    "#from app.fire import fire\n",
    "#from elapsedtimer import ElapsedTimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "VideoCaptioningPreProcessing will extract the Dense features from the <br>\n",
    "images in the Video frame using a Pre-trained VGG16 CNN Model<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCaptioningPreProcessing:\n",
    "    \n",
    "    def __init__(self,video_dest,feat_dir,\n",
    "                 temp_dest,img_dim=224,channels=3,\n",
    "                 batch_size=128,frames_step=80):\n",
    "        \n",
    "        self.img_dim = img_dim\n",
    "        self.channels = channels\n",
    "        self.video_dest = video_dest\n",
    "        self.feat_dir = feat_dir\n",
    "        self.temp_dest = temp_dest\n",
    "        self.batch_cnn = batch_size\n",
    "        self.frames_step = frames_step\n",
    "        \n",
    "        \n",
    "    def video_to_frames(self,video):\n",
    "        os.chdir('C:/Project/YouTubeClips')\n",
    "        with open(os.devnull, \"w\") as ffmpeg_log:\n",
    "            if os.path.exists(self.temp_dest):\n",
    "                print(\" cleanup: \" + self.temp_dest + \"/\")\n",
    "                print(video)\n",
    "                shutil.rmtree(self.temp_dest)\n",
    "            os.makedirs(self.temp_dest)\n",
    "            video_to_frames_cmd = [\"ffmpeg\",                                       \n",
    "                                       '-y',\n",
    "                                       '-i', video,  \n",
    "                                       '-vf', \"scale=400:300\", \n",
    "                                       '-qscale:v', \"2\", \n",
    "                                       '{0}/%06d.jpg'.format(self.temp_dest)]\n",
    "            subprocess.call(video_to_frames_cmd,\n",
    "                            stdout=ffmpeg_log, stderr=ffmpeg_log, shell=True)\n",
    "                        \n",
    "# Load the pre-trained VGG16 Model and extract the dense features as output \n",
    "    def model_cnn_load():\n",
    "     model = VGG16(weights = \"imagenet\", include_top=True,input_shape = (img_dim,img_dim,channels))\n",
    "     out = model.layers[-2].output\n",
    "     model_final = Model(inputs=model.input,outputs=out)\n",
    "     return model_final  \n",
    "    \n",
    "    def load_image(self,path):\n",
    "        #os.chdir('C:/Project/TempImages2')\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img,(self.img_dim,self.img_dim))\n",
    "        return img \n",
    "\n",
    "# Extract the features from the pre-trained CNN \n",
    "    def extract_feats_pretrained_cnn(self):\n",
    "\n",
    "        model = model_cnn_load()\n",
    "        print('Model loaded')\n",
    "\n",
    "        if not os.path.isdir(self.feat_dir):\n",
    "            os.mkdir(self.feat_dir)\n",
    "        #print(\"save video feats to %s\" % (self.dir_feat))\n",
    "        video_list = glob.glob(os.path.join(self.video_dest, '*.avi'))\n",
    "       # print (video_list )\n",
    "\n",
    "        for video in tqdm(video_list):\n",
    "\n",
    "            video_id = video.split(\"/\")[-1].split(\".\")[0]\n",
    "            print(f'Processing video {video}')\n",
    "           #print(f'Processing video {video_id+'.avi'}\n",
    "\n",
    "            #self.dest = 'cnn_feat' + '_' + video_id\n",
    "            self.video_to_frames(video)\n",
    "          \n",
    "            image_list = sorted(glob.glob(os.path.join(self.temp_dest, '*.jpg')))\n",
    "            samples = np.round(np.linspace( 0, len(image_list) - 1,self.frames_step))\n",
    "            image_list =[image_list[int(sample)] for sample in samples]\n",
    "            images = np.zeros((len(image_list),self.img_dim,self.img_dim,\n",
    "                     self.channels))\n",
    "            for i in range(len(image_list)):\n",
    "                img = self.load_image(image_list[i])\n",
    "                images[i] = img\n",
    "            images = np.array(images)\n",
    "            print('Model predicting')\n",
    "            fc_feats = model.predict(images,batch_size=self.batch_cnn)\n",
    "            print('prediction finished')\n",
    "            img_feats = np.array(fc_feats)\n",
    "            outfile = os.path.join(self.feat_dir, video_id + '.npy')\n",
    "            print('features saved')\n",
    "            np.save(outfile, img_feats)\n",
    "            # cleanup\n",
    "            shutil.rmtree(self.temp_dest)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dest = r'C:\\Project\\YouTubeClips'\n",
    "feat_dir =  'C:\\\\Project\\\\features2'\n",
    "temp_dest = 'C:\\\\Project\\\\TempImages2\\\\'\n",
    "img_dim = 224\n",
    "channels=3\n",
    "batch_size=128\n",
    "batch_cnn =128\n",
    "frames_step=80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn_load():\n",
    "     model = VGG16(weights = \"imagenet\", include_top=True,input_shape = (img_dim,img_dim,channels))\n",
    "     out = model.layers[-2].output\n",
    "     model_final = Model(inputs=model.input,outputs=out)\n",
    "     return model_final  \n",
    "#model = model_cnn_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the video into image frames at a specified sampling rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1970 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video C:\\Project\\YouTubeClips\\-4wsuPCjDBc_5_15.avi\n",
      " cleanup: C:\\Project\\TempImages2\\/\n",
      "C:\\Project\\YouTubeClips\\-4wsuPCjDBc_5_15.avi\n",
      "Model predicting\n"
     ]
    }
   ],
   "source": [
    " extract_features = VideoCaptioningPreProcessing(video_dest=video_dest,feat_dir=feat_dir,temp_dest=temp_dest,img_dim=img_dim,channels=channels, batch_size=128, frames_step=80)\n",
    "#extract_features.model_cnn_load()\n",
    "extract_features.extract_feats_pretrained_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib import rnn\n",
    "#from app.fire import  fire\n",
    "#from elapsedtimer import ElapsedTimer\n",
    "from pathlib import Path\n",
    "print('tensorflow version:',tf.__version__)\n",
    "from IPython.core.debugger import Pdb\n",
    "ipdb = Pdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the CNN features of a video frames and passes it through Back to Back LSTMs(Sequence to Sequence\n",
    "Model) to generate the Caption for the Video\n",
    "path_prj - Project directory.\n",
    "feat_dir - Subdirectory containing the CNN features .. absolute path /path_prj/feat_dir/\n",
    "cnn_feat_dim - Dimension of the feature vector from CNN for each image frame \n",
    "video_steps - No of image frames from each video. \n",
    "out_steps - Sequence length for the text caption. The output text sequence would be contained in 2o words.\n",
    "learning rate - training hyper parameter\n",
    "epoch - Traing epochs\n",
    "model_path - Absolute Path to save the model \n",
    "mode - train/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCaptioning:\n",
    "    \n",
    "    \n",
    "    def __init__(self,path_prj,caption_file,feat_dir,\n",
    "                 cnn_feat_dim=4096,h_dim=512,\n",
    "                 lstm_steps=80,video_steps=80,\n",
    "                 out_steps=20, frame_step=80,\n",
    "                 batch_size=8,learning_rate=1e-4,\n",
    "                 epochs=100,model_path=None,\n",
    "                 mode='train'):\n",
    "        self.dim_image = cnn_feat_dim\n",
    "        self.dim_hidden = h_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_steps = lstm_steps\n",
    "        self.video_lstm_step=video_steps\n",
    "        self.caption_lstm_step=out_steps\n",
    "        self.path_prj = Path(path_prj)\n",
    "        self.mode = mode\n",
    "        if mode == 'train':\n",
    "            self.train_text_path = self.path_prj / caption_file\n",
    "            self.train_feat_path = self.path_prj / feat_dir\n",
    "        else:\n",
    "            self.test_text_path = self.path_prj / caption_file\n",
    "            self.test_feat_path = self.path_prj / feat_dir\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.frame_step = frame_step\n",
    "        self.model_path = model_path\n",
    "    def build_model(self):\n",
    "\n",
    "        # Defining the weights associated with the Network\n",
    "        with tf.device('/cpu:0'): \n",
    "            self.word_emb = tf.Variable(tf.random_uniform([self.n_words, self.dim_hidden], -0.1, 0.1), name='word_emb')\n",
    "        self.lstm1 = tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "        self.lstm2 = tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "        self.encode_W = tf.Variable( tf.random_uniform([self.dim_image,self.dim_hidden], -0.1, 0.1), name='encode_W')\n",
    "        self.encode_b = tf.Variable( tf.zeros([self.dim_hidden]), name='encode_b')\n",
    "        \n",
    "        self.word_emb_W = tf.Variable(tf.random_uniform([self.dim_hidden,self.n_words], -0.1,0.1), name='word_emb_W')\n",
    "        self.word_emb_b = tf.Variable(tf.zeros([self.n_words]), name='word_emb_b')\n",
    "        \n",
    "        # Placeholders \n",
    "        video = tf.placeholder(tf.float32, [self.batch_size, self.video_lstm_step, self.dim_image])\n",
    "        video_mask = tf.placeholder(tf.float32, [self.batch_size, self.video_lstm_step])\n",
    "        caption = tf.placeholder(tf.int32, [self.batch_size, self.caption_lstm_step+1])\n",
    "        caption_mask = tf.placeholder(tf.float32, [self.batch_size, self.caption_lstm_step+1])\n",
    "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
    "        image_emb = tf.nn.xw_plus_b( video_flat, self.encode_W,self.encode_b )         \n",
    "        image_emb = tf.reshape(image_emb, [self.batch_size, self.lstm_steps, self.dim_hidden])\n",
    "        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n",
    "        padding = tf.zeros([self.batch_size, self.dim_hidden])\n",
    "        probs = []\n",
    "        loss = 0.0\n",
    "\n",
    "        #  Encoding Stage \n",
    "        for i in range(0, self.video_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(image_emb[:,i,:], state1)\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1],1), state2)\n",
    "\n",
    "        #  Decoding Stage  to generate Captions \n",
    "        for i in range(0, self.caption_lstm_step):\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.word_emb, caption[:, i])\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1],1), state2)\n",
    "            labels = tf.expand_dims(caption[:, i+1], 1)\n",
    "            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "            concated = tf.concat([indices, labels],1)\n",
    "            onehot_labels = tf.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "            logit_words = tf.nn.xw_plus_b(output2, self.word_emb_W, self.word_emb_b)\n",
    "        # Computing the loss     \n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words,labels=onehot_labels)\n",
    "            cross_entropy = cross_entropy * caption_mask[:,i]\n",
    "            probs.append(logit_words)\n",
    "            current_loss = tf.reduce_sum(cross_entropy)/self.batch_size\n",
    "            loss = loss + current_loss\n",
    "        with tf.variable_scope(tf.get_variable_scope(),reuse=tf.AUTO_REUSE):\n",
    "            train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)    \n",
    "        return loss, video, video_mask, caption, caption_mask, probs,train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_generator(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            self.word_emb = tf.Variable(tf.random_uniform([self.n_words, self.dim_hidden], -0.1, 0.1), name='word_emb')\n",
    "            self.lstm1 = tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "            self.lstm2 = tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "            self.encode_W = tf.Variable( tf.random_uniform([self.dim_image,self.dim_hidden], -0.1, 0.1), name='encode_W')\n",
    "            self.encode_b = tf.Variable( tf.zeros([self.dim_hidden]), name='encode_b')\n",
    "            self.word_emb_W = tf.Variable(tf.random_uniform([self.dim_hidden,self.n_words], -0.1,0.1), name='word_emb_W')\n",
    "            self.word_emb_b = tf.Variable(tf.zeros([self.n_words]), name='word_emb_b')\n",
    "            video = tf.placeholder(tf.float32, [1, self.video_lstm_step, self.dim_image])\n",
    "            video_mask = tf.placeholder(tf.float32, [1, self.video_lstm_step])\n",
    "            video_flat = tf.reshape(video, [-1, self.dim_image])\n",
    "            image_emb = tf.nn.xw_plus_b(video_flat, self.encode_W, self.encode_b)\n",
    "            image_emb = tf.reshape(image_emb, [1, self.video_lstm_step, self.dim_hidden])\n",
    "            state1 = tf.zeros([1, self.lstm1.state_size])\n",
    "            state2 = tf.zeros([1, self.lstm2.state_size])\n",
    "            padding = tf.zeros([1, self.dim_hidden])\n",
    "            generated_words = []\n",
    "            probs = []\n",
    "            embeds = []\n",
    "            for i in range(0, self.video_lstm_step):\n",
    "                if i > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                with tf.variable_scope(\"LSTM1\"):\n",
    "                    output1, state1 = self.lstm1(image_emb[:, i, :], state1)\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    output2, state2 = self.lstm2(tf.concat([padding, output1],1), state2)\n",
    "            for i in range(0, self.caption_lstm_step):\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                if i == 0:\n",
    "                    with tf.device('/cpu:0'):\n",
    "                        current_embed = tf.nn.embedding_lookup(self.word_emb, tf.ones([1], dtype=tf.int64))\n",
    "                with tf.variable_scope(\"LSTM1\"):\n",
    "                    output1, state1 = self.lstm1(padding, state1)\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    output2, state2 = self.lstm2(tf.concat([current_embed, output1],1), state2)\n",
    "                logit_words = tf.nn.xw_plus_b( output2, self.word_emb_W, self.word_emb_b)\n",
    "                max_prob_index = tf.argmax(logit_words, 1)[0]\n",
    "                generated_words.append(max_prob_index)\n",
    "                probs.append(logit_words)\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    current_embed = tf.nn.embedding_lookup(self.word_emb, max_prob_index)\n",
    "                    current_embed = tf.expand_dims(current_embed, 0)\n",
    "                embeds.append(current_embed)\n",
    "            return video, video_mask, generated_words, probs, embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the video images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
