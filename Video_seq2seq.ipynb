{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib import rnn\n",
    "#from app.fire import  fire\n",
    "#from elapsedtimer import ElapsedTimer\n",
    "from pathlib import Path\n",
    "print('tensorflow version:',tf.__version__)\n",
    "from IPython.core.debugger import Pdb\n",
    "ipdb = Pdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "This takes the CNN features of a video frames and passes it through Back to Back LSTMs(Sequence to Sequence<br>\n",
    "Model) to generate the Caption for the Video<br>\n",
    " path_prj - Project directory.<br>\n",
    " feat_dir - Subdirectory containing the CNN features .. absolute path /path_prj/feat_dir/<br>\n",
    " cnn_feat_dim - Dimension of the feature vector from CNN for each image frame <br>\n",
    " video_steps -  No of image frames from each video. <br>\n",
    " out_steps  - Sequence length for the text caption. The output text sequence would be contained in 2o words.<br>\n",
    " learning rate - training hyper parameter<br>\n",
    " epoch     - Traing epochs<br>\n",
    " model_path - Absolute Path to save the model <br>\n",
    " mode - train/inference <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCaptioning:\n",
    "    \n",
    "    \n",
    "    def __init__(self,path_prj,caption_file,feat_dir,\n",
    "                 cnn_feat_dim=4096,h_dim=512,\n",
    "                 lstm_steps=80,video_steps=80,\n",
    "                 out_steps=20, frame_step=80,\n",
    "                 batch_size=8,learning_rate=1e-4,\n",
    "                 epochs=100,model_path=None,\n",
    "                 mode='train'):\n",
    "        self.dim_image = cnn_feat_dim\n",
    "        self.dim_hidden = h_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_steps = lstm_steps\n",
    "        self.video_lstm_step=video_steps\n",
    "        self.caption_lstm_step=out_steps\n",
    "        self.path_prj = Path(path_prj)\n",
    "        self.mode = mode\n",
    "        if mode == 'train':\n",
    "            self.train_text_path = self.path_prj / caption_file\n",
    "            self.train_feat_path = self.path_prj / feat_dir\n",
    "        else:\n",
    "            self.test_text_path = self.path_prj / caption_file\n",
    "            self.test_feat_path = self.path_prj / feat_dir\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.frame_step = frame_step\n",
    "        self.model_path = model_path\n",
    "    def build_model(self):\n",
    "\n",
    "        # Defining the weights associated with the Network\n",
    "        with tf.device('/cpu:0'): \n",
    "            self.word_emb = tf.Variable(tf.random_uniform([self.n_words, self.dim_hidden], -0.1, 0.1), name='word_emb')\n",
    "        self.lstm1 = tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "        self.lstm2 = tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "        self.encode_W = tf.Variable( tf.random_uniform([self.dim_image,self.dim_hidden], -0.1, 0.1), name='encode_W')\n",
    "        self.encode_b = tf.Variable( tf.zeros([self.dim_hidden]), name='encode_b')\n",
    "        \n",
    "        self.word_emb_W = tf.Variable(tf.random_uniform([self.dim_hidden,self.n_words], -0.1,0.1), name='word_emb_W')\n",
    "        self.word_emb_b = tf.Variable(tf.zeros([self.n_words]), name='word_emb_b')\n",
    "        \n",
    "        # Placeholders \n",
    "        video = tf.placeholder(tf.float32, [self.batch_size, self.video_lstm_step, self.dim_image])\n",
    "        video_mask = tf.placeholder(tf.float32, [self.batch_size, self.video_lstm_step])\n",
    "        caption = tf.placeholder(tf.int32, [self.batch_size, self.caption_lstm_step+1])\n",
    "        caption_mask = tf.placeholder(tf.float32, [self.batch_size, self.caption_lstm_step+1])\n",
    "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
    "        image_emb = tf.nn.xw_plus_b( video_flat, self.encode_W,self.encode_b )         \n",
    "        image_emb = tf.reshape(image_emb, [self.batch_size, self.lstm_steps, self.dim_hidden])\n",
    "        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n",
    "        padding = tf.zeros([self.batch_size, self.dim_hidden])\n",
    "        probs = []\n",
    "        loss = 0.0\n",
    "\n",
    "        #  Encoding Stage \n",
    "        for i in range(0, self.video_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(image_emb[:,i,:], state1)\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1],1), state2)\n",
    "\n",
    "        #  Decoding Stage  to generate Captions \n",
    "        for i in range(0, self.caption_lstm_step):\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.word_emb, caption[:, i])\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1],1), state2)\n",
    "            labels = tf.expand_dims(caption[:, i+1], 1)\n",
    "            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "            concated = tf.concat([indices, labels],1)\n",
    "            onehot_labels = tf.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "            logit_words = tf.nn.xw_plus_b(output2, self.word_emb_W, self.word_emb_b)\n",
    "        # Computing the loss     \n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words,labels=onehot_labels)\n",
    "            cross_entropy = cross_entropy * caption_mask[:,i]\n",
    "            probs.append(logit_words)\n",
    "            current_loss = tf.reduce_sum(cross_entropy)/self.batch_size\n",
    "            loss = loss + current_loss\n",
    "        with tf.variable_scope(tf.get_variable_scope(),reuse=tf.AUTO_REUSE):\n",
    "            train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)    \n",
    "        return loss, video, video_mask, caption, caption_mask, probs,train_op\n",
    "    \n",
    "    def build_generator(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "        self.word_emb = tf.Variable(tf.random_uniform([self.n_words, self.dim_hidden], -0.1, 0.1), name='word_emb')\n",
    "        self.lstm1 = tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "        self.lstm2 = tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "        self.encode_W = tf.Variable( tf.random_uniform([self.dim_image,self.dim_hidden], -0.1, 0.1), name='encode_W')\n",
    "        self.encode_b = tf.Variable( tf.zeros([self.dim_hidden]), name='encode_b')\n",
    "        self.word_emb_W = tf.Variable(tf.random_uniform([self.dim_hidden,self.n_words], -0.1,0.1), name='word_emb_W')\n",
    "        self.word_emb_b = tf.Variable(tf.zeros([self.n_words]), name='word_emb_b')\n",
    "        video = tf.placeholder(tf.float32, [1, self.video_lstm_step, self.dim_image])\n",
    "        video_mask = tf.placeholder(tf.float32, [1, self.video_lstm_step])\n",
    "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
    "        image_emb = tf.nn.xw_plus_b(video_flat, self.encode_W, self.encode_b)\n",
    "        image_emb = tf.reshape(image_emb, [1, self.video_lstm_step, self.dim_hidden])\n",
    "        state1 = tf.zeros([1, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([1, self.lstm2.state_size])\n",
    "        padding = tf.zeros([1, self.dim_hidden])\n",
    "        generated_words = []\n",
    "        probs = []\n",
    "        embeds = []\n",
    "        for i in range(0, self.video_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(image_emb[:, i, :], state1)\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1],1), state2)\n",
    "        for i in range(0, self.caption_lstm_step):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            if i == 0:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    current_embed = tf.nn.embedding_lookup(self.word_emb, tf.ones([1], dtype=tf.int64))\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1],1), state2)\n",
    "            logit_words = tf.nn.xw_plus_b( output2, self.word_emb_W, self.word_emb_b)\n",
    "            max_prob_index = tf.argmax(logit_words, 1)[0]\n",
    "            generated_words.append(max_prob_index)\n",
    "            probs.append(logit_words)\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.word_emb, max_prob_index)\n",
    "                current_embed = tf.expand_dims(current_embed, 0)\n",
    "            embeds.append(current_embed)\n",
    "        return video, video_mask, generated_words, probs, embeds     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
